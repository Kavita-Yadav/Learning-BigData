###############
#### FLUME ####
###############

Helps in data streaming

#### What is Flume ?
- Another way to stream data into your cluster.
- Made from the start with Hadoop in mind.
    - Built-in sinks for HDFS and Hbase.
- Originally made to handle log aggregation.

#### Anatomy of a Flume Agent and Flow

          Web servers ----> Source                 Sink ----> HBase
                                  \               /|\
                                    \ ---      ,---
                                        \|/   /
                                       Channel
                           
- Source 
    - where the data is coming from.
    - Can optionally have Channel Selector and Interceptors.
- Channel
    - How data get transferred between source to sink(two ways to transfer: through memory which is faster or through files).
- Sink 
    - Determine where your data gets stored to or where your data is going. 
    - There can be multiple sinks as per requirements.You can organize multiple sink into Sink Groups. 
    - But remember sink can connect with only one channel.
        - Channel is notified to delete a message once the sink processes it.
      
#### Difference between kafka and flume:
- Kafka just stores up data indefinitely and people can pull that data whenever they want to. 
- Kafka does expire date after some amount of time, but with flume your sink grabs data from flume and once it's been grabbed
  from the channel it gets deleted.
- Kafka is little more easier to set up if you have data going to multipe different places that might be pulling at different
  rates.
  
#### Built-in Source Types in flume
- Spooling directory
    * Can monitor directory for log files or any files that gets dropped into it and use as data source.
- Avro
    * A communication formay that's specific to Hadoop.
- Kafka
    * Can connect to Kafka as a source.
- Exec
    * Can connect to Exec means can connect to output of any command line prompt. Like, could do a tail dash off on a log file
      and feed new lines that get appended to file into flume as a source.
- Thrift
    * Like Avro, another data connection interface that can use to tie different agents together.
- Netcat
    * Can be used to actually listen to data being streamed in any arbitrary TCP port.
- HTTP
    * Source type that just listens to web traffic; and you can write your own source from scratch as well in java if you want to.
- Custom
    * Flume can also integrate with custom server.
- And many more !

#### Built-in Sink Types in flume:
- HDFS
- Hive
- HBase
- Avro
- Thrift
- Elasticsearch
- Kafka
- Custom
- And more!

#### Using Avro, agents can connect to other agents as well

            App Server \              
                   \     \ --->       _____________
                     \ ------->      | Flume Agent |
                     / \ ,---->       _____________ \  _____________
            App Server  /\                            | Flume Agent | ----> HDFS(or something)
                     \ /  \---->      _____________ /  _____________
                      /-------->     | Flume Agent |
                     /   ,----->      _____________
                    /   / 
           App Server_/
       
  There is lot of log traffic generated by application servers that dumps its data into first tier flume agent and 
  then it will pass to the final tier which actually writes into HDFS. It benefits if you are dealing with multiple 
  data centers, etc.
  
  #### Create Simple Flow:
  
                            Source:netcat               Sink: Logger 
                                        \               /|\
                                          \ ---      ,---
                                              \|/   /
                                             Channel:
                                             memory
                           
       * Log spool to HDFS
  
             Files ----> Source: spooldir/                Sink:HDFS ----> HDFS
                            TimeStamp    \               /|\
                            interceptor   \ ---      ,---
                                              \|/   /
                                              Channel:
                                              memory

#### Set up Flume and publish logs with it:
- Login to hadoop terminal as maria_dev
- wget media.sundog-soft.com/hadoop/example.conf
- Now, open second hadoop terminal.
- cd /usr/hdp/current/flume-server/
- bin/flume-ng agent --conf conf --conf-file ~/example.conf --name a1 -Dflume.root.logger=INFO,console
- In first terminal: 

     $ telnet localhost 44444
 
     $ Hello there, how are you doing?
     
     $ Forescore and seven years ago
     
     Ctrl + c
     
 - quit
 
 #### Set up Flume to monitor a directory and store its data in HDFS
 - In first terminal,
 
     $ pwd
     
     $ wget media.sundog-soft.com/hadoop/flumelogs.conf
        
 - mkdir spool
 
 - Login to Ambari.
 
 - Go to FileView -> user-> maria_dev -> NewFolder -> name: flume
 
 - In second terminal, 
 
    $ bin/flume-ng agent --conf conf --conf-file ~/flumelogs.conf --name a1 -Dflume.root.logger=INFO,console
    
 - In first terminal,
 
    $ cp access_log_small.txt spool/fred.txt
    
    $ cd spool
    
 - Go to Ambari to see the logs -> user -> maria_dev -> flume -> 17-01-14(current date) -> 2240(current time) -> 00
 
 - In second terminal, 
 
     $ Ctrl + C 
     
     $ quit
     
 - In first terminal, $ exit and ACPI shutdown.
